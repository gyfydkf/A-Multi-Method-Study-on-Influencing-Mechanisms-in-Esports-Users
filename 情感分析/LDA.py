import pandas as pd
import jieba
from gensim import corpora, models
import pyLDAvis
import pyLDAvis.gensim_models
import re
import webbrowser
import os

# é…ç½®æ–‡ä»¶è·¯å¾„
file_path = 'merged_output.csv'
stopwords_path = 'cn_stopwords.txt'  # è¯·æå‰ä¸‹è½½å¥½ï¼Œæ”¾åœ¨åŒç›®å½•

# 1. è¯»å–æ•°æ®ï¼Œæ£€æµ‹ç¼–ç 
def read_csv_safely(file_path):
    for enc in ['utf-8', 'utf-8-sig', 'latin1']:
        try:
            df = pd.read_csv(file_path, encoding=enc)
            print(f"âœ… ä½¿ç”¨ç¼–ç  {enc} æˆåŠŸè¯»å–æ•°æ®ï¼Œå…± {len(df)} è¡Œ")
            print(df.head())
            return df
        except Exception as e:
            print(f"âš ï¸ å°è¯•ä½¿ç”¨ç¼–ç  {enc} è¯»å–å¤±è´¥: {e}")
    print("âŒ æ‰€æœ‰ç¼–ç æ–¹å¼è¯»å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼")
    exit()

df = read_csv_safely(file_path)

# æ£€æŸ¥ 'text' åˆ—æ˜¯å¦å­˜åœ¨
if 'text' not in df.columns:
    print("âŒ åŸå§‹æ•°æ®ä¸­ä¸å­˜åœ¨ 'text' åˆ—ï¼Œè¯·æ£€æŸ¥åˆ—åæ˜¯å¦æ­£ç¡®ï¼")
    exit()

# 2. æ–‡æœ¬é¢„å¤„ç†ï¼šå»é™¤ç¼ºå¤±å€¼ï¼Œæ¸…æ´—æ–‡æœ¬
texts = df['text'].dropna().astype(str).tolist()
print(f"âœ… åˆ é™¤ç¼ºå¤±å€¼åæ–‡æœ¬æ•°é‡ï¼š{len(texts)}")

def clean_text(text):
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•°å­—ï¼Œå»é™¤å…¶ä»–ç¬¦å·
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', text)
    return text

texts = [clean_text(text) for text in texts]
print(f"âœ… æ–‡æœ¬æ¸…æ´—åç¤ºä¾‹ï¼š{texts[:5]}")

# 3. åˆ†è¯å¤„ç†
tokenized_texts = [list(jieba.cut(text)) for text in texts]
print(f"âœ… åˆ†è¯åç¤ºä¾‹ï¼š{tokenized_texts[:5]}")

# 4. åœç”¨è¯å¤„ç†ï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰
def load_stopwords(filepath):
    if not os.path.exists(filepath):
        print(f"âš ï¸ åœç”¨è¯æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{filepath}ï¼Œè¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„ï¼")
        return set()
    try:
        stopwords = set(pd.read_csv(filepath, header=None, quoting=3, encoding='utf-8')[0])
        print(f"âœ… åœç”¨è¯åŠ è½½æˆåŠŸï¼Œæ•°é‡ï¼š{len(stopwords)}")
    except Exception as e:
        print(f"âš ï¸ åœç”¨è¯åŠ è½½å¤±è´¥: {e}")
        stopwords = set()
    return stopwords

stop_words = load_stopwords(stopwords_path)

# å»é™¤åœç”¨è¯å’Œé•¿åº¦å°äºç­‰äº 1 çš„è¯è¯­
tokenized_texts = [
    [word for word in doc if word not in stop_words and len(word) > 1]
    for doc in tokenized_texts
]
print(f"âœ… å»é™¤åœç”¨è¯åç¤ºä¾‹ï¼š{tokenized_texts[:5]}")

# æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç©ºæ–‡æ¡£
non_empty_texts = [doc for doc in tokenized_texts if doc]
if not non_empty_texts:
    print("âŒ é¢„å¤„ç†åæ–‡æœ¬ä¸ºç©ºï¼Œå¯èƒ½æ˜¯æ•°æ®è¿‡å°‘æˆ–è¿‡æ»¤æ¡ä»¶è¿‡ä¸¥ã€‚è¯·æ£€æŸ¥åŸå§‹æ•°æ®å’Œé¢„å¤„ç†æ­¥éª¤ï¼")
    exit()

# 5. åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“
dictionary = corpora.Dictionary(non_empty_texts)
corpus = [dictionary.doc2bow(text) for text in non_empty_texts]
print(f"âœ… è¯å…¸å¤§å°ï¼š{len(dictionary)}")
print(f"âœ… è¯­æ–™åº“æ ·æœ¬æ•°ï¼š{len(corpus)}")

# æ£€æŸ¥è¯å…¸å’Œè¯­æ–™åº“æ˜¯å¦æœ‰æ•ˆ
if len(dictionary) == 0 or len(corpus) == 0:
    print("âŒ å­—å…¸æˆ–è¯­æ–™åº“ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ LDA æ¨¡å‹ã€‚è¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤ï¼")
    exit()

# 6. è®­ç»ƒ LDA æ¨¡å‹
num_topics = 5  # ä¸»é¢˜æ•°ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
lda_model = models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=num_topics,
    random_state=42,
    passes=10
)
print("âœ… LDA æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

# 7. è¾“å‡ºæ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
print("\nğŸ‰ ä¸»é¢˜å…³é”®è¯ï¼š")
for idx, topic in lda_model.print_topics(num_words=10):
    print(f"ä¸»é¢˜ {idx + 1}: {topic}")

# 8. å¯è§†åŒ–
# å‡†å¤‡å¯è§†åŒ–æ•°æ®
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)

# ä¿å­˜ä¸º HTML æ–‡ä»¶
html_path = 'lda_visualization.html'
pyLDAvis.save_html(vis, html_path)
print(f"\nâœ… LDA å¯è§†åŒ–å·²ä¿å­˜ä¸º {html_path}")

# è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨æ˜¾ç¤ºç»“æœ
webbrowser.open(html_path)

# 9. æ¯æ¡è¯„è®ºçš„ä¸»é¢˜åˆ†å¸ƒ
doc_topics = []
for doc in corpus:
    topic_probs = lda_model.get_document_topics(doc, minimum_probability=0)
    doc_topics.append([prob for _, prob in topic_probs])

topic_df = pd.DataFrame(doc_topics, columns=[f'Topic {i + 1}' for i in range(num_topics)])
print("\nâœ… æ¯æ¡è¯„è®ºçš„ä¸»é¢˜åˆ†å¸ƒ (å‰ 5 è¡Œ)ï¼š")
print(topic_df.head())

# ä¿å­˜ä¸»é¢˜åˆ†å¸ƒç»“æœåˆ° CSV
topic_df.to_csv('topic_distribution.csv', index=False)
print("âœ… æ¯æ¡è¯„è®ºçš„ä¸»é¢˜åˆ†å¸ƒå·²ä¿å­˜ä¸º topic_distribution.csv")
